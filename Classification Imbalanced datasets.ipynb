{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling imbalanced datasets in machine learning\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from sklearn import cluster\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "creditData=np.loadtxt('creditcard.csv',dtype=np.str,delimiter=',',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(creditData.shape)\n",
    "\n",
    "#stripping \"\" from output variable\n",
    "creditData[:,creditData.shape[1]-1]=np.core.defchararray.strip(creditData[:,creditData.shape[1]-1], chars='\"')\n",
    "#type conversion\n",
    "creditData=creditData.astype(np.float)\n",
    "print(creditData.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001727485630620034\n"
     ]
    }
   ],
   "source": [
    "#1. Data Level Approach\n",
    "fraudRate=(np.count_nonzero(creditData[:,creditData.shape[1]-1])/creditData.shape[0])\n",
    "print(fraudRate)#ratio of non fraud to fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33000, 30) (33000, 1) (3000, 31) (30000, 31) (96, 31) (96, 31)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'#tsne\\nXnew=TSNE(n_components=2).fit_transform(xTrain)\\n#print(\"K means for t-SNE\")\\n\\n#kmeans = KMeans(n_clusters=2).fit(Xnew)\\n#y=kmeans.predict(Xnew)\\nfig = plt.figure(figsize=(10,10))\\n#matplotlib.pyplot.scatter(Xnew[:,0], Xnew[:,1], c=y, cmap=\\'tab10\\')\\ncolors = [\\'blue\\',\\'red\\']\\nmatplotlib.pyplot.scatter(Xnew[:,0], Xnew[:,1], c=np.ravel(yTrain.astype(np.int)), cmap=matplotlib.colors.ListedColormap(colors))\\n#plt.title(\\'Clustering of points after running the kmeans algorithm on tSNE redued data\\')\\nplt.xlabel(\\'x1\\')\\nplt.ylabel(\\'x2\\')\\nplt.show()'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#splitting between train and test set\n",
    "xTrain, xTest, yTrain, yTest=sklearn.model_selection.train_test_split(creditData[:,:creditData.shape[1]-1],creditData[:,creditData.shape[1]-1],test_size=0.20)\n",
    "\n",
    "#test contains test input and output both\n",
    "test=pd.DataFrame(xTest)\n",
    "test[test.shape[1]]=yTest\n",
    "testF=test[test[test.shape[1]-1]==1]#test data for fraud examples\n",
    "testNf=test[test[test.shape[1]-1]==0]#test data for non fraud examples\n",
    "\n",
    "\n",
    "train=pd.DataFrame(xTrain)\n",
    "train[train.shape[1]]=yTrain\n",
    "trainF=train[train[train.shape[1]-1]==1]#train data for fraud examples\n",
    "trainNf=train[train[train.shape[1]-1]==0]#train data for non fraud examples\n",
    "\n",
    "#reducing the size of data\n",
    "train=pd.DataFrame()\n",
    "xTrain=pd.DataFrame()\n",
    "yTrain=pd.DataFrame()\n",
    "for i in range(30000):\n",
    "    r=np.random.randint(0,trainNf.shape[0])\n",
    "    train=train.append(trainNf.iloc[r,:])  \n",
    "for i in range(3000):\n",
    "    r=np.random.randint(0,trainF.shape[0])\n",
    "    train=train.append(trainF.iloc[r,:])\n",
    "\n",
    "xTrain=xTrain.append(train.loc[:,:train.shape[1]-2])\n",
    "yTrain[train.shape[1]-1]=train.loc[:,train.shape[1]-1]\n",
    "\n",
    "trainF=train[train[train.shape[1]-1]==1]\n",
    "trainNf=train[train[train.shape[1]-1]==0]\n",
    "\n",
    "testN=pd.DataFrame()\n",
    "for i in range(testF.shape[0]):\n",
    "    r=np.random.randint(0,testNf.shape[0])\n",
    "    testN=testN.append(testNf.iloc[r,:])\n",
    "testNf=testN\n",
    "print(xTrain.shape,yTrain.shape,trainF.shape,trainNf.shape,testF.shape,testNf.shape)\n",
    "\n",
    "'''#tsne\n",
    "Xnew=TSNE(n_components=2).fit_transform(xTrain)\n",
    "#print(\"K means for t-SNE\")\n",
    "\n",
    "#kmeans = KMeans(n_clusters=2).fit(Xnew)\n",
    "#y=kmeans.predict(Xnew)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#matplotlib.pyplot.scatter(Xnew[:,0], Xnew[:,1], c=y, cmap='tab10')\n",
    "colors = ['blue','red']\n",
    "matplotlib.pyplot.scatter(Xnew[:,0], Xnew[:,1], c=np.ravel(yTrain.astype(np.int)), cmap=matplotlib.colors.ListedColormap(colors))\n",
    "#plt.title('Clustering of points after running the kmeans algorithm on tSNE redued data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random undersampling\n",
    "train=pd.DataFrame()\n",
    "for i in range(np.count_nonzero(yTrain)):\n",
    "    r=np.random.randint(0,trainNf.shape[0])\n",
    "    train=train.append(trainNf.iloc[r,:])  \n",
    "train=train.append(trainF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.43564356435643564\n",
      "0.6435643564356436\n",
      "0.43564356435643564\n",
      "0.6435643564356436\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(bad classification with eucledian distance metric)\n",
    "\n",
    "clf=sklearn.neighbors.NearestCentroid().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7920792079207921\n",
      "precision  1.0\n",
      "F1 score  0.8839779005524863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance with p=-1)\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7326732673267327\n",
      "precision  1.0\n",
      "F1 score  0.8457142857142858\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7722772277227723\n",
      "precision  1.0\n",
      "F1 score  0.8715083798882681\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8811881188118812\n",
      "precision  1.0\n",
      "F1 score  0.9368421052631579\n",
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.8811881188118812\n",
      "precision  0.9888888888888889\n",
      "F1 score  0.9319371727748692\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8415841584158416\n",
      "precision  1.0\n",
      "F1 score  0.913978494623656\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier(class_weight={1:10}).fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.3125\n",
      "precision  0.967741935483871\n",
      "F1 score  0.47244094488188976\n",
      "accuracy on non fraud samples  0.90625\n",
      "recall  0.7708333333333334\n",
      "precision  0.891566265060241\n",
      "F1 score  0.8268156424581005\n"
     ]
    }
   ],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precision\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.8712871287128713\n",
      "precision  0.9887640449438202\n",
      "F1 score  0.9263157894736842\n",
      "accuracy on non fraud samples  0.9702970297029703\n",
      "recall  0.9405940594059405\n",
      "98.0 95.0 3.0\n",
      "precision  0.9693877551020408\n",
      "F1 score  0.9547738693467337\n",
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.9405940594059405\n",
      "precision  0.9895833333333334\n",
      "F1 score  0.964467005076142\n"
     ]
    }
   ],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(TN,TP,FP)\n",
    "print(\"precision \",precision)#precision\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "clf=linear_model.LogisticRegression(class_weight={1:10}).fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n",
      "[LibSVM]accuracy on non fraud samples  0.43564356435643564\n",
      "recall  0.6336633663366337\n",
      "precision  0.5289256198347108\n",
      "F1 score  0.5765765765765766\n",
      "[LibSVM]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.9207920792079208\n",
      "precision  1.0\n",
      "F1 score  0.9587628865979382\n"
     ]
    }
   ],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "clf=sklearn.svm.SVC(kernel='linear',class_weight={1:10},cache_size=4096,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Oversampling\n",
    "\n",
    "train=pd.DataFrame()\n",
    "for i in range(yTrain.shape[0]-2*np.count_nonzero(yTrain)):\n",
    "    r=np.random.randint(0,trainF.shape[0])\n",
    "    train=train.append(trainF.iloc[r,:])  \n",
    "train=train.append(trainNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7326732673267327\n",
      "precision  1.0\n",
      "F1 score  0.8457142857142858\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7524752475247525\n",
      "precision  1.0\n",
      "F1 score  0.8587570621468926\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8712871287128713\n",
      "precision  1.0\n",
      "F1 score  0.9312169312169313\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8613861386138614\n",
      "precision  1.0\n",
      "F1 score  0.9255319148936171\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.3125\n",
      "precision  0.967741935483871\n",
      "F1 score  0.47244094488188976\n",
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.3541666666666667\n",
      "precision  0.9714285714285714\n",
      "F1 score  0.5190839694656488\n"
     ]
    }
   ],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.8712871287128713\n",
      "precision  0.9887640449438202\n",
      "F1 score  0.9263157894736842\n",
      "accuracy on non fraud samples  0.9801980198019802\n",
      "recall  0.9405940594059405\n",
      "precision  0.979381443298969\n",
      "F1 score  0.9595959595959594\n"
     ]
    }
   ],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n",
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "               \n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6000, 31)\n"
     ]
    }
   ],
   "source": [
    "#k means on non fraud data with number of centres=number of fraud data\n",
    "clf=cluster.KMeans(n_clusters=trainF.shape[0]).fit(trainNf.loc[:,:trainNf.shape[1]-2])\n",
    "train=pd.DataFrame(clf.cluster_centers_)\n",
    "train[train.shape[1]]=np.zeros(train.shape[0])\n",
    "train=train.append(trainF)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8217821782178217\n",
      "precision  1.0\n",
      "F1 score  0.9021739130434783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7326732673267327\n",
      "precision  1.0\n",
      "F1 score  0.8457142857142858\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8811881188118812\n",
      "precision  1.0\n",
      "F1 score  0.9368421052631579\n",
      "accuracy on non fraud samples  0.7920792079207921\n",
      "recall  0.9207920792079208\n",
      "precision  0.8157894736842105\n",
      "F1 score  0.8651162790697674\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.3125\n",
      "precision  0.967741935483871\n",
      "F1 score  0.47244094488188976\n",
      "accuracy on non fraud samples  0.8541666666666666\n",
      "recall  0.9166666666666666\n",
      "precision  0.8627450980392157\n",
      "F1 score  0.888888888888889\n"
     ]
    }
   ],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.8712871287128713\n",
      "precision  0.9887640449438202\n",
      "F1 score  0.9263157894736842\n",
      "accuracy on non fraud samples  0.9504950495049505\n",
      "recall  0.9306930693069307\n",
      "precision  0.9494949494949495\n",
      "F1 score  0.9400000000000001\n"
     ]
    }
   ],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n",
      "[LibSVM]accuracy on non fraud samples  0.6336633663366337\n",
      "recall  0.26732673267326734\n",
      "precision  0.421875\n",
      "F1 score  0.32727272727272727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "0.0    30400\n",
      "1.0    46640\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#cluster based oversampling\n",
    "dataCluster=pd.DataFrame()\n",
    "dataCluster=dataCluster.append(trainNf)\n",
    "#making clusters\n",
    "clf1=cluster.KMeans(n_clusters=20).fit(trainNf.loc[:,:trainNf.shape[1]-2])\n",
    "clusterTotal=pd.DataFrame(clf1.predict(trainNf.loc[:,:trainNf.shape[1]-2]))\n",
    "dataCluster[31]=np.array(clusterTotal)\n",
    "clusterTotal1=clusterTotal.groupby([0]).size()\n",
    "clusterMax1=clusterTotal1.max()\n",
    "\n",
    "clf2=cluster.KMeans(n_clusters=2).fit(trainF.loc[:,:trainF.shape[1]-2])\n",
    "clusterTotal=pd.DataFrame(clf2.predict(trainF.loc[:,:trainF.shape[1]-2]))\n",
    "clusterTotal2=clusterTotal.groupby([0]).size()\n",
    "clusterMax2=20*clusterMax1//2\n",
    "\n",
    "#oversampling data of each cluster\n",
    "\n",
    "dataCluster=dataCluster.sort_values(by=trainNf.shape[1]-1)\n",
    "train=pd.DataFrame()\n",
    "train=train.append(trainNf)\n",
    "currind=0\n",
    "for i in range(20):\n",
    "    for j in (clusterMax1-clusterTotal1):\n",
    "        r=np.random.randint(0,clusterTotal1[i])\n",
    "        train=train.append(dataCluster.iloc[r+currind,:31])\n",
    "    currind=currind+clusterTotal1[i]\n",
    "#print(train.head)\n",
    "\n",
    "dataCluster=pd.DataFrame(trainF)\n",
    "dataCluster=dataCluster.sort_values(by=trainF.shape[1]-1)\n",
    "train=train.append(trainF)\n",
    "currind=0\n",
    "for i in range(2):\n",
    "    for j in range(clusterMax2):\n",
    "        r=np.random.randint(0,clusterTotal2[i])\n",
    "        train=train.append(dataCluster.iloc[r+currind,:31])\n",
    "    currind=currind+clusterTotal2[i]\n",
    "print(train.groupby([train.shape[1]-1]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7326732673267327\n",
      "precision  1.0\n",
      "F1 score  0.8457142857142858\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7821782178217822\n",
      "precision  1.0\n",
      "F1 score  0.8777777777777778\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8811881188118812\n",
      "precision  1.0\n",
      "F1 score  0.9368421052631579\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8415841584158416\n",
      "precision  1.0\n",
      "F1 score  0.913978494623656\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.3125\n",
      "precision  0.967741935483871\n",
      "F1 score  0.47244094488188976\n",
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.53125\n",
      "precision  0.9807692307692307\n",
      "F1 score  0.6891891891891891\n"
     ]
    }
   ],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.8712871287128713\n",
      "precision  0.9887640449438202\n",
      "F1 score  0.9263157894736842\n",
      "accuracy on non fraud samples  0.9801980198019802\n",
      "recall  0.9405940594059405\n",
      "precision  0.979381443298969\n",
      "F1 score  0.9595959595959594\n"
     ]
    }
   ],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None) \n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data \n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]) )\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n",
      "[LibSVM]accuracy on non fraud samples  0.45544554455445546\n",
      "recall  0.6336633663366337\n",
      "precision  0.5378151260504201\n",
      "F1 score  0.5818181818181819\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#SVM\n",
    "\n",
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3000, 31)\n",
      "(30000, 31)\n",
      "(60000, 31)\n"
     ]
    }
   ],
   "source": [
    "#smote\n",
    "train=pd.DataFrame(trainF)\n",
    "print(train.shape)\n",
    "#print(trainF.iloc[0,:trainF.shape[1]-2])\n",
    "for i in range(yTrain.shape[0]-2*np.count_nonzero(yTrain)):\n",
    "    j=np.random.randint(0,trainF.shape[0])\n",
    "    neigh=sklearn.neighbors.KNeighborsClassifier(n_neighbors=1).fit(trainF.loc[:,:trainF.shape[1]-2],trainF.loc[:,trainF.shape[1]-1])\n",
    "    neighbour=neigh.kneighbors(np.reshape(np.array(trainF.iloc[j,:trainF.shape[1]-1]),(1,-1)), 2, False)\n",
    "    p=np.random.random()\n",
    "    newPoint=(trainF.iloc[j,:trainF.shape[1]-1])+p*(trainF.iloc[neighbour[0][1],:trainF.shape[1]-1]-trainF.iloc[j,:trainF.shape[1]-1])\n",
    "    newPoint=newPoint.append(pd.Series(1))\n",
    "    newPoint=newPoint.reset_index(drop=True)\n",
    "    train=train.append(newPoint,ignore_index=True)\n",
    "print(train.shape)\n",
    "train=train.append(trainNf)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8118811881188119\n",
      "precision  1.0\n",
      "F1 score  0.8961748633879781\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8217821782178217\n",
      "precision  1.0\n",
      "F1 score  0.9021739130434783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7326732673267327\n",
      "precision  1.0\n",
      "F1 score  0.8457142857142858\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7623762376237624\n",
      "precision  1.0\n",
      "F1 score  0.8651685393258427\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8811881188118812\n",
      "precision  1.0\n",
      "F1 score  0.9368421052631579\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.8415841584158416\n",
      "precision  1.0\n",
      "F1 score  0.913978494623656\n"
     ]
    }
   ],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in power\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.3125\n",
      "precision  0.967741935483871\n",
      "F1 score  0.47244094488188976\n",
      "accuracy on non fraud samples  0.9895833333333334\n",
      "recall  0.53125\n",
      "precision  0.9807692307692307\n",
      "F1 score  0.6891891891891891\n"
     ]
    }
   ],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9900990099009901\n",
      "recall  0.8712871287128713\n",
      "precision  0.9887640449438202\n",
      "F1 score  0.9263157894736842\n",
      "accuracy on non fraud samples  0.9702970297029703\n",
      "recall  0.9405940594059405\n",
      "precision  0.9693877551020408\n",
      "F1 score  0.9547738693467337\n"
     ]
    }
   ],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n",
      "[LibSVM]accuracy on non fraud samples  0.5544554455445545\n",
      "recall  0.36633663366336633\n",
      "precision  0.45121951219512196\n",
      "F1 score  0.40437158469945356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:218: ConvergenceWarning: Solver terminated early (max_iter=200).  Consider pre-processing your data with StandardScaler or MinMaxScaler.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
