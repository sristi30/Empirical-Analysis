{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Handling imbalanced datasets in machine learning\n",
    "\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "import math\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "import random\n",
    "from sklearn import cluster\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading data\n",
    "\n",
    "creditData=np.loadtxt('creditcard.csv',dtype=np.str,delimiter=',',skiprows=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(284807, 31)\n",
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(creditData.shape)\n",
    "\n",
    "#stripping \"\" from output variable\n",
    "creditData[:,creditData.shape[1]-1]=np.core.defchararray.strip(creditData[:,creditData.shape[1]-1], chars='\"')\n",
    "#type conversion\n",
    "creditData=creditData.astype(np.float)\n",
    "print(creditData.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001727485630620034\n"
     ]
    }
   ],
   "source": [
    "#1. Data Level Approach\n",
    "fraudRate=(np.count_nonzero(creditData[:,creditData.shape[1]-1])/creditData.shape[0])\n",
    "print(fraudRate)#ratio of non fraud to fraud data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-7c6fe76f8f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mxTrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxTrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1470\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1471\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2013\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_valid_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2014\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2015\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_lowerdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_has_valid_tuple\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    220\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIndexingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Too many indexers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 raise ValueError(\"Location based indexing can only have \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_key\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1955\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1956\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1957\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1958\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m             \u001b[0;31m# a tuple should already have been caught by this point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   2007\u001b[0m         \u001b[0ml\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2009\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"single positional indexer is out-of-bounds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2010\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2011\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "#splitting between train and test set\n",
    "xTrain, xTest, yTrain, yTest=sklearn.model_selection.train_test_split(creditData[:,:creditData.shape[1]-1],creditData[:,creditData.shape[1]-1],test_size=0.20)\n",
    "\n",
    "#test contains test input and output both\n",
    "test=pd.DataFrame(xTest)\n",
    "test[test.shape[1]]=yTest\n",
    "testF=test[test[test.shape[1]-1]==1]#test data for fraud examples\n",
    "testNf=test[test[test.shape[1]-1]==0]#test data for non fraud examples\n",
    "\n",
    "\n",
    "train=pd.DataFrame(xTrain)\n",
    "train[train.shape[1]]=yTrain\n",
    "trainF=train[train[train.shape[1]-1]==1]#train data for fraud examples\n",
    "trainNf=train[train[train.shape[1]-1]==0]#train data for non fraud examples\n",
    "\n",
    "#reducing the size of data\n",
    "train=pd.DataFrame()\n",
    "xTrain=pd.DataFrame()\n",
    "yTrain=pd.DataFrame()\n",
    "for i in range(30000):\n",
    "    r=random.randint(0,trainNf.shape[0])\n",
    "    train=train.append(trainNf.iloc[r,:])  \n",
    "for i in range(300):\n",
    "    r=random.randint(0,trainF.shape[0])\n",
    "    train=train.append(trainF.iloc[r,:])\n",
    "\n",
    "xTrain=xTrain.append(train.loc[:,:train.shape[1]-2])\n",
    "yTrain[train.shape[1]-1]=train.loc[:,train.shape[1]-1]\n",
    "\n",
    "trainF=train[train[train.shape[1]-1]==1]\n",
    "trainNf=train[train[train.shape[1]-1]==0]\n",
    "\n",
    "testN=pd.DataFrame()\n",
    "for i in range(testF.shape[0]):\n",
    "    r=random.randint(0,testNf.shape[0])\n",
    "    testN=testN.append(testNf.iloc[r,:])\n",
    "testNf=testN\n",
    "print(xTrain.shape,yTrain.shape,trainF.shape,trainNf.shape,testF.shape,testNf.shape)\n",
    "\n",
    "#tsne\n",
    "Xnew=TSNE(n_components=2).fit_transform(xTrain)\n",
    "#print(\"K means for t-SNE\")\n",
    "\n",
    "#kmeans = KMeans(n_clusters=2).fit(Xnew)\n",
    "#y=kmeans.predict(Xnew)\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "#matplotlib.pyplot.scatter(Xnew[:,0], Xnew[:,1], c=y, cmap='tab10')\n",
    "colors = ['blue','red']\n",
    "matplotlib.pyplot.scatter(Xnew[:,0], Xnew[:,1], c=np.ravel(yTrain.astype(np.int)), cmap=matplotlib.colors.ListedColormap(colors))\n",
    "#plt.title('Clustering of points after running the kmeans algorithm on tSNE redued data')\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3030, 2) (3030, 1)\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "print(Xnew.shape,yTrain.shape)\n",
    "print(np.ravel(yTrain.astype(np.int)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random undersampling\n",
    "train=pd.DataFrame()\n",
    "for i in range(np.count_nonzero(yTrain)):\n",
    "    r=np.random.randint(0,trainNf.shape[0])\n",
    "    train=train.append(trainNf.iloc[r,:])  \n",
    "train=train.append(trainF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.48357715571810383\n",
      "0.6777777777777778\n",
      "0.4844035729357153\n",
      "0.6777777777777778\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(bad classification with eucledian distance metric)\n",
    "\n",
    "clf=sklearn.neighbors.NearestCentroid().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9992966661977775\n",
      "recall  0.8\n",
      "precision  0.6428571428571429\n",
      "F1 score  0.7128712871287128\n",
      "accuracy on non fraud samples  0.999437332958222\n",
      "recall  0.8\n",
      "precision  0.6923076923076923\n",
      "F1 score  0.7422680412371134\n"
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance with p=-1)\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9928435785623857\n",
      "recall  0.7333333333333333\n",
      "precision  0.13953488372093023\n",
      "F1 score  0.23445825932504438\n",
      "accuracy on non fraud samples  0.9890983260655507\n",
      "recall  0.7666666666666667\n",
      "precision  0.10014513788098693\n",
      "F1 score  0.17715019255455713\n"
     ]
    }
   ],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier(class_weight={1:10}).fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precision\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(TN,TP,FP)\n",
    "print(\"precision \",precision)#precision\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "clf=linear_model.LogisticRegression(class_weight={1:10}).fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096,max_iter=200,verbose=True).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "clf=sklearn.svm.SVC(kernel='linear',class_weight={1:10},cache_size=4096,verbose=True).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Oversampling\n",
    "\n",
    "train=pd.DataFrame()\n",
    "for i in range(yTrain.shape[0]-2*np.count_nonzero(yTrain)):\n",
    "    r=np.random.randint(0,trainF.shape[0])\n",
    "    train=train.append(trainF.iloc[r,:])  \n",
    "train=train.append(trainNf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "               \n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#k means on non fraud data with number of centres=number of fraud data\n",
    "clf=cluster.KMeans(n_clusters=trainF.shape[0]).fit(trainNf.loc[:,:trainNf.shape[1]-2])\n",
    "train=pd.DataFrame(clf.cluster_centers_)\n",
    "train[train.shape[1]-1]=np.zeros(train.shape[0])\n",
    "train=train.append(trainF)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  1.0\n",
      "recall  0.696078431372549\n",
      "precision  1.0\n",
      "F1 score  0.8208092485549133\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.696078431372549\n",
      "precision  1.0\n",
      "F1 score  0.8208092485549133\n",
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.7058823529411765\n",
      "precision  0.9863013698630136\n",
      "F1 score  0.8228571428571428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py:140: UserWarning: Averaging for metrics other than euclidean and manhattan not supported. The average is set to be the mean.\n",
      "  warnings.warn(\"Averaging for metrics other than \"\n",
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: RuntimeWarning: divide by zero encountered in power\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.7058823529411765\n",
      "precision  0.9863013698630136\n",
      "F1 score  0.8228571428571428\n",
      "accuracy on non fraud samples  0.9803921568627451\n",
      "recall  0.8137254901960784\n",
      "precision  0.9764705882352941\n",
      "F1 score  0.8877005347593584\n",
      "accuracy on non fraud samples  0.9117647058823529\n",
      "recall  0.9509803921568627\n",
      "precision  0.9150943396226415\n",
      "F1 score  0.9326923076923077\n",
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.8137254901960784\n",
      "precision  0.9880952380952381\n",
      "F1 score  0.8924731182795699\n",
      "accuracy on non fraud samples  0.9607843137254902\n",
      "recall  0.6764705882352942\n",
      "precision  0.9452054794520548\n",
      "F1 score  0.7885714285714286\n",
      "accuracy on non fraud samples  0.9607843137254902\n",
      "recall  0.7941176470588235\n",
      "precision  0.9529411764705882\n",
      "F1 score  0.8663101604278074\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.6862745098039216\n",
      "precision  1.0\n",
      "F1 score  0.813953488372093\n",
      "accuracy on non fraud samples  0.9705882352941176\n",
      "recall  0.8725490196078431\n",
      "99.0 89.0 3.0\n",
      "precision  0.967391304347826\n",
      "F1 score  0.9175257731958762\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7941176470588235\n",
      "precision  1.0\n",
      "F1 score  0.8852459016393442\n",
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.4215686274509804\n",
      "precision  0.9772727272727273\n",
      "F1 score  0.589041095890411\n",
      "accuracy on non fraud samples  0.7549019607843137\n",
      "recall  0.8431372549019608\n",
      "precision  0.7747747747747747\n",
      "F1 score  0.8075117370892019\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7843137254901961\n",
      "precision  1.0\n",
      "F1 score  0.8791208791208792\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.696078431372549\n",
      "precision  1.0\n",
      "F1 score  0.8208092485549133\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7156862745098039\n",
      "precision  1.0\n",
      "F1 score  0.8342857142857142\n",
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.7058823529411765\n",
      "precision  0.9863013698630136\n",
      "F1 score  0.8228571428571428\n",
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.7549019607843137\n",
      "precision  0.9871794871794872\n",
      "F1 score  0.8555555555555555\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.7450980392156863\n",
      "precision  1.0\n",
      "F1 score  0.8539325842696629\n",
      "accuracy on non fraud samples  0.9705882352941176\n",
      "recall  0.7843137254901961\n",
      "precision  0.963855421686747\n",
      "F1 score  0.8648648648648649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lenovo/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:207: RuntimeWarning: divide by zero encountered in power\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy on non fraud samples  0.9607843137254902\n",
      "recall  0.6764705882352942\n",
      "precision  0.9452054794520548\n",
      "F1 score  0.7885714285714286\n",
      "accuracy on non fraud samples  0.9313725490196079\n",
      "recall  0.7843137254901961\n",
      "precision  0.9195402298850575\n",
      "F1 score  0.8465608465608465\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.6862745098039216\n",
      "precision  1.0\n",
      "F1 score  0.813953488372093\n",
      "accuracy on non fraud samples  0.9803921568627451\n",
      "recall  0.8921568627450981\n",
      "precision  0.978494623655914\n",
      "F1 score  0.9333333333333333\n",
      "accuracy on non fraud samples  0.9901960784313726\n",
      "recall  0.4215686274509804\n",
      "precision  0.9772727272727273\n",
      "F1 score  0.589041095890411\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.6862745098039216\n",
      "precision  1.0\n",
      "F1 score  0.813953488372093\n",
      "(60, 31)\n",
      "accuracy on non fraud samples  1.0\n",
      "recall  0.696078431372549\n",
      "precision  1.0\n",
      "F1 score  0.8208092485549133\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-509b850c7e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;31m#prototype based classification on random undersampled data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m \u001b[0mclf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneighbors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNearestCentroid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetric\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"accuracy on non fraud samples \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestNf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestNf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestNf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestNf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#accuracy on non fraud examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recall \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtestF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtestF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtestF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#accuracy on fraud examples(recall)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/neighbors/nearest_centroid.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    103\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'csr'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'csc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m         \u001b[0mis_X_sparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_X_sparse\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrink_threshold\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m         \u001b[0m_assert_all_finite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_numeric\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'O'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     42\u001b[0m             and not np.isfinite(X).all()):\n\u001b[1;32m     43\u001b[0m         raise ValueError(\"Input contains NaN, infinity\"\n\u001b[0;32m---> 44\u001b[0;31m                          \" or a value too large for %r.\" % X.dtype)\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster based oversampling\n",
    "dataCluster=pd.DataFrame()\n",
    "dataCluster=dataCluster.append(trainNf)\n",
    "#making clusters\n",
    "clf1=cluster.KMeans(n_clusters=20).fit(trainNf.loc[:,:trainNf.shape[1]-2])\n",
    "clusterTotal=pd.DataFrame(clf1.predict(trainNf.loc[:,:trainNf.shape[1]-2]))\n",
    "dataCluster[31]=np.array(clusterTotal)\n",
    "clusterTotal1=clusterTotal.groupby([0]).size()\n",
    "clusterMax1=clusterTotal1.max()\n",
    "\n",
    "clf2=cluster.KMeans(n_clusters=2).fit(trainF.loc[:,:trainF.shape[1]-2])\n",
    "clusterTotal=pd.DataFrame(clf2.predict(trainF.loc[:,:trainF.shape[1]-2]))\n",
    "clusterTotal2=clusterTotal.groupby([0]).size()\n",
    "clusterMax2=20*clusterMax1//2\n",
    "\n",
    "#oversampling data of each cluster\n",
    "\n",
    "dataCluster=dataCluster.sort_values(by=trainNf.shape[1]-1)\n",
    "train=pd.DataFrame()\n",
    "train=train.append(trainNf)\n",
    "currind=0\n",
    "for i in range(20):\n",
    "    for j in (clusterMax1-clusterTotal1):\n",
    "        r=np.random.randint(0,clusterTotal1[i])\n",
    "        train=train.append(dataCluster.iloc[r+currind,:31])\n",
    "    currind=currind+clusterTotal1[i]\n",
    "#print(train.head)\n",
    "\n",
    "dataCluster=pd.DataFrame(trainF)\n",
    "dataCluster=dataCluster.sort_values(by=trainF.shape[1]-1)\n",
    "train=train.append(trainF)\n",
    "currind=0\n",
    "for i in range(2):\n",
    "    for j in range(clusterMax2):\n",
    "        r=np.random.randint(0,clusterTotal2[i])\n",
    "        train=train.append(dataCluster.iloc[r+currind,:31])\n",
    "    currind=currind+clusterTotal2[i]\n",
    "print(train.groupby([train.shape[1]-1]).size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None) \n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data \n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]) )\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM\n",
    "\n",
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#smote\n",
    "train=pd.DataFrame(trainF)\n",
    "print(train.shape)\n",
    "#print(trainF.iloc[0,:trainF.shape[1]-2])\n",
    "for i in range(yTrain.shape[0]-2*np.count_nonzero(yTrain)):\n",
    "    j=np.random.randint(0,trainF.shape[0])\n",
    "    neigh=sklearn.neighbors.KNeighborsClassifier(n_neighbors=1).fit(trainF.loc[:,:trainF.shape[1]-2],trainF.loc[:,trainF.shape[1]-1])\n",
    "    neighbour=neigh.kneighbors(np.reshape(np.array(trainF.iloc[j,:trainF.shape[1]-1]),(1,-1)), 2, False)\n",
    "    p=np.random.random()\n",
    "    newPoint=(trainF.iloc[j,:trainF.shape[1]-1])+p*(trainF.iloc[neighbour[0][1],:trainF.shape[1]-1]-trainF.iloc[j,:trainF.shape[1]-1])\n",
    "    newPoint=newPoint.append(pd.Series(1))\n",
    "    newPoint=newPoint.reset_index(drop=True)\n",
    "    train=train.append(newPoint,ignore_index=True)\n",
    "print(train.shape)\n",
    "train=train.append(trainNf)\n",
    "print(train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prototype based classification on original data(classification with minkowski distance)\n",
    "\n",
    "\n",
    "def dist(x,y):\n",
    "    x=abs(x-y)\n",
    "    x=x**(-0.1)\n",
    "    s=np.sum(x)\n",
    "    s=s**(-1/0.1)\n",
    "    return s\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#prototype based classification on random undersampled data\n",
    "clf=sklearn.neighbors.NearestCentroid(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive bayes classifier\n",
    "\n",
    "clf=GaussianNB().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#naive bayes classification on random undersampled data\n",
    "clf=GaussianNB().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Decision tree classifier\n",
    "\n",
    "clf = sklearn.tree.DecisionTreeClassifier().fit(xTrain,yTrain)\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#decision tree classification on random undersampled data\n",
    "clf=sklearn.tree.DecisionTreeClassifier().fit(train.loc[:,:train.shape[1]-2],train.loc[:,train.shape[1]-1])\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K- nearest neighbor\n",
    "\n",
    "clf = sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#K nearest neighbor classification on random undersampled data\n",
    "clf=sklearn.neighbors.KNeighborsClassifier(metric=dist).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logistic regression on original data\n",
    "#class sklearn.linear_model.LogisticRegression(penalty=’l2’, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver=’warn’, max_iter=100, multi_class=’warn’, verbose=0, warm_start=False, n_jobs=None)\n",
    "clf=linear_model.LogisticRegression().fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#logistic regression on random undersampled data\n",
    "clf=linear_model.LogisticRegression().fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM on original data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(xTrain,np.ravel(yTrain))\n",
    "\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n",
    "\n",
    "#SVM on random undersampled data\n",
    "clf=sklearn.svm.SVC(kernel='linear',cache_size=4096).fit(train.loc[:,:train.shape[1]-2],np.ravel(train.loc[:,train.shape[1]-1]))\n",
    "print(\"accuracy on non fraud samples \",clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1]))#accuracy on non fraud examples\n",
    "print(\"recall \", clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))#accuracy on fraud examples(recall)\n",
    "TP=clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])*(testF.shape[0])\n",
    "TN=clf.score(testNf.loc[:,:testNf.shape[1]-2],testNf.loc[:,testNf.shape[1]-1])*(testNf.shape[0])\n",
    "FP=(testNf.shape[0])-TN\n",
    "precision=(TP)/(TP+FP)\n",
    "print(\"precision \",precision)#precisio\n",
    "print(\"F1 score \",2*precision*clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1])/(precision+(clf.score(testF.loc[:,:testF.shape[1]-2],testF.loc[:,testF.shape[1]-1]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
